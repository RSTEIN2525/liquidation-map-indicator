import time
import threading
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from apscheduler.schedulers.background import BackgroundScheduler
from typing import Optional, List
import pandas as pd
from google.cloud import storage
import json
import os
from supabase import create_client, Client

from .models import CacheStatus, LiquidationMapResponse, RawLiquidation
from .main import calculate_map_data
from .config import validate_ticker, validate_exchanges, validate_lookback
from datetime import datetime, timedelta

# Configuration
BUCKET_NAME = "liquidation-cache-crypto-dash-482023"
STORAGE_CLIENT = storage.Client()
CACHE_BLOB_NAME = "latest_map.json"

# Supabase Config
SUPABASE_URL = os.environ.get("SUPABASE_PROJECT_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_API_KEY")
supabase: Optional[Client] = None

if SUPABASE_URL and SUPABASE_KEY:
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("âœ… Supabase client initialized")
    except Exception as e:
        print(f"âš ï¸ Failed to initialize Supabase: {e}")
else:
    print("âš ï¸ Supabase credentials not found (SUPABASE_URL, SUPABASE_KEY)")

# Lightweight status tracking only (no data stored in memory)
CACHE_STATUS = {"status": CacheStatus.INITIALIZING}

def save_to_gcs(data: LiquidationMapResponse):
    """Persists the cache to Google Cloud Storage (no local copy)"""
    try:
        bucket = STORAGE_CLIENT.bucket(BUCKET_NAME)
        blob = bucket.blob(CACHE_BLOB_NAME)
        # Use model_dump_json() for Pydantic V2
        blob.upload_from_string(
            data.model_dump_json(), 
            content_type='application/json'
        )
        print("âœ… Cache persisted to GCS")
        return True
    except Exception as e:
        print(f"âŒ GCS Save failed: {e}")
        return False

def load_from_gcs() -> LiquidationMapResponse | None:
    """Load the cache directly from GCS (no memory caching)"""
    try:
        bucket = STORAGE_CLIENT.bucket(BUCKET_NAME)
        blob = bucket.blob(CACHE_BLOB_NAME)
        if blob.exists():
            content = blob.download_as_text()
            response = LiquidationMapResponse.model_validate_json(content)
            return response
    except Exception as e:
        print(f"âš ï¸ GCS Load failed: {e}")
    return None

def save_prediction_to_supabase(response_model: LiquidationMapResponse):
    """Save the new prediction and full report to Supabase"""
    if not supabase:
        return

    try:
        summary = response_model.summary
        direction = response_model.direction
        
        # We need to serialize the model to a dict for JSONB storage
        report_json = json.loads(response_model.model_dump_json())
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'bias': direction.bias,
            'upward_mag': direction.upward_mag,
            'downward_mag': direction.downward_mag,
            'price_at_prediction': summary.close,
            'symbol': 'BTC',
            'timeframe': '1h',
            'report_data': report_json  # Store the full historical snapshot!
        }
        
        supabase.table('predictions').insert(data).execute()
        print("âœ… Prediction & Report saved to Supabase")
        
    except Exception as e:
        print(f"âŒ Failed to save prediction: {e}")

def backfill_actuals():
    """Check old predictions and grade them against current price"""
    if not supabase:
        return

    try:
        # Get predictions from >1 hour ago that haven't been graded
        cutoff = (datetime.now() - timedelta(hours=1)).isoformat()
        
        response = supabase.table('predictions') \
            .select('*') \
            .is_('price_1h_later', 'null') \
            .lte('timestamp', cutoff) \
            .execute()
        
        predictions_to_grade = response.data
        
        if not predictions_to_grade:
            return

        print(f"ðŸ“ Grading {len(predictions_to_grade)} predictions...")
        
        # We need current price. We can get it from the cache or fetch fresh
        # For simplicity, let's assume the cache is reasonably fresh
        current_data = load_from_gcs()
        if not current_data:
            print("âš ï¸ Cannot grade: No current price available")
            return
            
        current_price = current_data.summary.close
        
        for pred in predictions_to_grade:
            # Calculate result
            start_price = pred['price_at_prediction']
            price_change_pct = ((current_price - start_price) / start_price) * 100
            
            # Determine if prediction was correct
            # UP bias means we want price to go UP
            # DOWN bias means we want price to go DOWN
            # UNBIASED ... well, maybe we skip grading or check for low volatility?
            
            direction_correct = None
            if pred['bias'] == 'UP':
                direction_correct = price_change_pct > 0
            elif pred['bias'] == 'DOWN':
                direction_correct = price_change_pct < 0
            
            # Update the record
            supabase.table('predictions').update({
                'price_1h_later': current_price,
                'price_change_pct': price_change_pct,
                'direction_correct': direction_correct
            }).eq('id', pred['id']).execute()
            
        print(f"âœ… Graded {len(predictions_to_grade)} predictions")

    except Exception as e:
        print(f"âŒ Grading failed: {e}")

def update_cache():
    """Main loop: Fetch from exchanges and save to GCS (no memory caching)"""
    try:
        print("ðŸ”„ Updating cache...")
        # Call Main Sequence (The heavy lifting)
        result = calculate_map_data()

        # Grade old predictions (do this first to keep logic clean)
        backfill_actuals()

        # Validate Data Present
        if not result['bins'].empty:
            # Prepare bins for Pydantic serialization
            bins_df = result['bins'].copy()
            bins_df['bucket'] = bins_df['bucket'].astype(str)
            
            # Prepare raw liquidations (convert timestamp to unix time)
            raw_liqs_list = None
            if 'raw_liqs' in result and not result['raw_liqs'].empty:
                raw_df = result['raw_liqs'].copy()
                # Convert entry_start_time to unix timestamp if it exists
                if 'entry_start_time' in raw_df.columns:
                    raw_df['entry_time'] = raw_df['entry_start_time'].astype('int64') // 10**9
                else:
                    raw_df['entry_time'] = None
                
                raw_liqs_list = [
                    RawLiquidation(
                        price=row['price'],
                        usd=row['usd'],
                        side=row['side'],
                        status=row['status'],
                        entry_time=row.get('entry_time')
                    ) for _, row in raw_df.iterrows()
                ]
            
            # Construct the DTO
            response = LiquidationMapResponse(
                summary=result['summary'],
                direction=result['direction'],
                bins=bins_df.to_dict(orient='records'),
                raw_liquidations=raw_liqs_list,
                timestamp=time.time()
            )

            # Save prediction AND full report to Supabase
            save_prediction_to_supabase(response)

            # Persist to GCS (no memory copy)
            if save_to_gcs(response):
                CACHE_STATUS["status"] = CacheStatus.READY
                print(f"âœ… Cache updated successfully at {pd.Timestamp.now()}")
            else:
                CACHE_STATUS["status"] = CacheStatus.ERROR

    except Exception as e:
        print(f"âŒ Update Failed: {e}")
        import traceback
        traceback.print_exc()
        CACHE_STATUS["status"] = CacheStatus.ERROR

@asynccontextmanager
async def lifespan(app: FastAPI):
    # 1. Check if cache exists in GCS
    print("ðŸš€ Application startup...")
    cached_data = load_from_gcs()
    if cached_data:
        CACHE_STATUS["status"] = CacheStatus.READY
        print("âœ… Found existing cache in GCS")
    else:
        CACHE_STATUS["status"] = CacheStatus.INITIALIZING
        print("âš ï¸ No cache found, will initialize in background")

    # 2. Setup Background Scheduler for regular updates
    scheduler = BackgroundScheduler()
    scheduler.add_job(update_cache, 'interval', hours=1)
    scheduler.start()
    print("â° Scheduler started (updates every 1 hour)")

    # 3. Run initial update in background (if no cache or to refresh)
    thread = threading.Thread(target=update_cache, daemon=True)
    thread.start()
    print("ðŸ”„ Background update started")

    yield

    # On Shutdown
    print("ðŸ›‘ Shutting down scheduler...")
    scheduler.shutdown()

# Define APP
app = FastAPI(lifespan=lifespan)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",  # Vite dev server
        "http://localhost:3000",  # Next.js dev server
        "http://localhost:8080",  # Common dev port
        "https://www.coinmag.xyz",
    ],
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods (GET, POST, etc.)
    allow_headers=["*"],  # Allows all headers
)

# ========== The Endpoints ========== #
@app.get("/api/status")
def get_status():
    """Simple check to see if cache is ready"""
    return {"status": CACHE_STATUS["status"]}

@app.get("/api/liquidation-map", response_model=LiquidationMapResponse)
def get_liquidation_map():
    """Get the full dataset for the UI (cached, reads from GCS)"""
    # Always read fresh from GCS (no memory cache)
    data = load_from_gcs()
    
    if data is None:
        raise HTTPException(
            status_code=503, 
            detail={
                "error": "Data is warming up, please wait...", 
                "status": CACHE_STATUS["status"]
            }
        )

    return data

@app.get("/api/liquidation-map/custom", response_model=LiquidationMapResponse)
def get_custom_liquidation_map(
    ticker: Optional[str] = Query(default="BTC", description="Ticker symbol (BTC, ETH, SOL, BNB, XRP, DOGE, ADA)"),
    lookback_days: Optional[float] = Query(default=14.0, description="Lookback period in days (0.5 = 12hr, 1 = 1 day, 7 = 1 week, 30 = 1 month)"),
    exchanges: Optional[str] = Query(default=None, description="Comma-separated list of exchanges (e.g., 'binance,bybit,okx')")
):
    """
    Get liquidation map with custom parameters (NOT CACHED - may be slower).
    
    **Note**: This endpoint calculates fresh data on each request and does not use the cache.
    Expect response times of 10-30 seconds depending on exchange availability.
    
    ### Parameters:
    - **ticker**: BTC, ETH, SOL, BNB, XRP, DOGE, ADA
    - **lookback_days**: 0.5 (12hr), 1 (1 day), 7 (1 week), 30 (1 month)
    - **exchanges**: Comma-separated list from: binance, bybit, okx, hyperliquid, mexc, krakenfutures, kucoinfutures, gateio, bitget, deribit
    
    ### Example:
    ```
    /api/liquidation-map/custom?ticker=ETH&lookback_days=7&exchanges=binance,bybit,okx
    ```
    """
    try:
        # Validate and parse inputs
        ticker = validate_ticker(ticker)
        lookback_days = validate_lookback(lookback_days)
        
        exchange_list = None
        if exchanges:
            exchange_list = [ex.strip().lower() for ex in exchanges.split(',')]
            exchange_list = validate_exchanges(exchange_list)
            if not exchange_list:
                raise HTTPException(status_code=400, detail="No valid exchanges provided")
        
        # Calculate fresh data
        result = calculate_map_data(
            ticker=ticker,
            exchanges=exchange_list,
            lookback_days=lookback_days
        )
        
        # Prepare response
        bins_df = result['bins'].copy()
        bins_df['bucket'] = bins_df['bucket'].astype(str)
        
        # Prepare raw liquidations
        raw_liqs_list = None
        if 'raw_liqs' in result and not result['raw_liqs'].empty:
            raw_df = result['raw_liqs'].copy()
            # Convert entry_start_time to unix timestamp if it exists
            if 'entry_start_time' in raw_df.columns:
                raw_df['entry_time'] = raw_df['entry_start_time'].astype('int64') // 10**9
            else:
                raw_df['entry_time'] = None
            
            raw_liqs_list = [
                RawLiquidation(
                    price=row['price'],
                    usd=row['usd'],
                    side=row['side'],
                    status=row['status'],
                    entry_time=row.get('entry_time')
                ) for _, row in raw_df.iterrows()
            ]
        
        response = LiquidationMapResponse(
            summary=result['summary'],
            direction=result['direction'],
            bins=bins_df.to_dict(orient='records'),
            raw_liquidations=raw_liqs_list,
            timestamp=time.time()
        )
        
        return response
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to calculate custom map: {str(e)}"
        )

@app.post("/api/admin/update")
def trigger_update(secret: str = Query(..., description="Secret key for authorization")):
    """
    Manually trigger cache update (for Cloud Scheduler or manual testing).
    
    This endpoint runs the update synchronously, ensuring Cloud Run waits
    for completion before shutting down the instance.
    
    Usage:
    ```
    curl -X POST "https://your-api.run.app/api/admin/update?secret=YOUR_SECRET"
    ```
    """
    admin_secret = os.environ.get("ADMIN_SECRET", "dev-secret-change-me")
    
    if secret != admin_secret:
        raise HTTPException(
            status_code=403,
            detail="Invalid secret key. Set ADMIN_SECRET environment variable."
        )
    
    # Run synchronously so Cloud Run waits for completion
    try:
        print("ðŸ”„ Manual update triggered via /api/admin/update")
        update_cache()
        return {
            "status": "success",
            "message": "Cache updated successfully",
            "timestamp": time.time()
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Update failed: {str(e)}"
        )
